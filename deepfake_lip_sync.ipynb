{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deepfake_lip_sync.ipynb","provenance":[{"file_id":"1tZpDWXz49W6wDcTprANRGLo2D_EbD5J8","timestamp":1621820748951},{"file_id":"1NLUwupCBsB1HrpEmOIHeMgU63sus2LxP","timestamp":1597735440478}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AYBXSU0U8s8z"},"source":["## Rithish Kumar\n","\n","###Credits to Wav2Lip pretrained model"]},{"cell_type":"code","metadata":{"id":"XIVB0Xn1g6ih","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993407686,"user_tz":-330,"elapsed":360,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"50df8069-3bd9-442e-b5eb-140b312dac1f"},"source":["!nvcc --version"],"execution_count":3,"outputs":[{"output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Wed_Jul_22_19:09:09_PDT_2020\n","Cuda compilation tools, release 11.0, V11.0.221\n","Build cuda_11.0_bu.TC445_37.28845127_0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yJ5taGmPcWV-"},"source":["# Getting Pretrained Wav2Lip program from git.\n","which can be used for academic and research purposes."]},{"cell_type":"code","metadata":{"id":"P3LihClHbUd3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993412939,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"c83a806a-e390-4c54-dab2-4a514a40f1a1"},"source":["!git clone https://github.com/Rudrabha/Wav2Lip.git"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'Wav2Lip'...\n","remote: Enumerating objects: 338, done.\u001b[K\n","remote: Total 338 (delta 0), reused 0 (delta 0), pack-reused 338\u001b[K\n","Receiving objects: 100% (338/338), 511.49 KiB | 11.12 MiB/s, done.\n","Resolving deltas: 100% (184/184), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SMC7BIdCTdPQ","executionInfo":{"status":"ok","timestamp":1621993417074,"user_tz":-330,"elapsed":36,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"18b1653d-06dc-4ca3-aa2f-280f65d921a8"},"source":["!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["sample_data  Wav2Lip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vB_NuW-VROz","executionInfo":{"status":"ok","timestamp":1621993431235,"user_tz":-330,"elapsed":5316,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"5fe4ba65-cbd3-49c3-98a3-43fef4a1e732"},"source":["#Downloading weight from drive\n","%cd  /content/Wav2Lip/checkpoints/\n","#https://drive.google.com/file/d/1Si-Z2YwQF2aHf2BPXWSMhqqh3uXQ1jq0/view?usp=sharing\n","!gdown https://drive.google.com/uc?id=1Si-Z2YwQF2aHf2BPXWSMhqqh3uXQ1jq0"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/Wav2Lip/checkpoints\n","Downloading...\n","From: https://drive.google.com/uc?id=1Si-Z2YwQF2aHf2BPXWSMhqqh3uXQ1jq0\n","To: /content/Wav2Lip/checkpoints/wav2lip_gan.pth\n","436MB [00:02, 203MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YjzMPy_Sb0AI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993469124,"user_tz":-330,"elapsed":357,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"9a6bb94b-7512-4899-f592-f7e335b38f09"},"source":["%cd /content/"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u-1O9NyQGMDR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993564133,"user_tz":-330,"elapsed":2212,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"2312d51b-fdad-4d40-ead4-d39853f6fcdb"},"source":["#Downloading face detection model\n","#https://drive.google.com/file/d/1Di135y6JGejcnUW2h6ioaSvLxodmKO6m/view?usp=sharing\n","\n","!gdown https://drive.google.com/uc?id=1Di135y6JGejcnUW2h6ioaSvLxodmKO6m"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1Di135y6JGejcnUW2h6ioaSvLxodmKO6m\n","To: /content/haarcascade_frontalface_alt2.xml\n","\r  0% 0.00/541k [00:00<?, ?B/s]\r100% 541k/541k [00:00<00:00, 74.8MB/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aWTaOS3ncFt6"},"source":["# Get the pre-requisites"]},{"cell_type":"code","metadata":{"id":"Ooh28vw-Uvd3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993608259,"user_tz":-330,"elapsed":36986,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"39a94e7f-6c32-4521-c795-e50b2d49db55"},"source":["!pip uninstall tensorflow tensorflow-gpu"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.5.0:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.7/dist-packages/tensorflow-2.5.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.5.0\n","\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"49dCYlLdcK2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993781220,"user_tz":-330,"elapsed":172970,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"636acb64-d7c1-439c-d1b2-28c2371f74b9"},"source":["!cd Wav2Lip && pip install -r requirements.txt"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Collecting librosa==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/6e/0eb0de1c9c4e02df0b40e56f258eb79bd957be79b918511a184268e01720/librosa-0.7.0.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 7.0MB/s \n","\u001b[?25hCollecting numpy==1.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/eb/4ecf6b13897391cb07a4231e9d9c671b55dfbbf6f4a514a1a0c594f2d8d9/numpy-1.17.1-cp37-cp37m-manylinux1_x86_64.whl (20.3MB)\n","\u001b[K     |████████████████████████████████| 20.3MB 1.4MB/s \n","\u001b[?25hCollecting opencv-contrib-python>=4.2.0.34\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/77/347867aff4a9a5b56ea587a50762fb0ba40b27bb4e8f8f27c25ba57ed807/opencv_contrib_python-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (57.4MB)\n","\u001b[K     |████████████████████████████████| 57.4MB 49kB/s \n","\u001b[?25hCollecting opencv-python==4.1.0.25\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/52/61b9619a7a95a8d809515f68f1441224a07ce1873fd3af5e662851014a55/opencv_python-4.1.0.25-cp37-cp37m-manylinux1_x86_64.whl (26.6MB)\n","\u001b[K     |████████████████████████████████| 26.6MB 115kB/s \n","\u001b[?25hCollecting torch==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/23/a4b5c189dd624411ec84613b717594a00480282b949e3448d189c4aa4e47/torch-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (676.9MB)\n","\u001b[K     |████████████████████████████████| 676.9MB 27kB/s \n","\u001b[?25hCollecting torchvision==0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/9b/208f48d5a5013bdb0c27a84a02df4fcf5fd24ab5902667c11e554a12b681/torchvision-0.3.0-cp37-cp37m-manylinux1_x86_64.whl (2.6MB)\n","\u001b[K     |████████████████████████████████| 2.6MB 34kB/s \n","\u001b[?25hCollecting tqdm==4.45.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n","\u001b[?25hCollecting numba==0.48\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/dc/5ce4a94d98e8a31cab21b150e23ca2f09a7dd354c06a69f71801ecd890db/numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5MB)\n","\u001b[K     |████████████████████████████████| 3.6MB 41.0MB/s \n","\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (2.1.9)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (4.4.2)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.2.2)\n","Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.7.0->-r requirements.txt (line 1)) (0.10.3.post1)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.3.0->-r requirements.txt (line 6)) (7.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48->-r requirements.txt (line 8)) (56.1.0)\n","Collecting llvmlite<0.32.0,>=0.31.0dev0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/10/d02c0ac683fc47ecda3426249509cf771d748b6a2c0e9d5ebbee76a7b80a/llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2MB)\n","\u001b[K     |████████████████████████████████| 20.2MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa==0.7.0->-r requirements.txt (line 1)) (1.14.5)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.0->-r requirements.txt (line 1)) (2.20)\n","Building wheels for collected packages: librosa\n","  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for librosa: filename=librosa-0.7.0-cp37-none-any.whl size=1598345 sha256=0211af5638aa03c2f25b8d6b48539e1a52a96c187200c97edbe9e2476d60e36f\n","  Stored in directory: /root/.cache/pip/wheels/49/1d/38/c8ad12fcad67569d8e730c3275be5e581bd589558484a0f881\n","Successfully built librosa\n","\u001b[31mERROR: kapre 0.3.5 requires tensorflow>=2.0.0, which is not installed.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement librosa>=0.7.2, but you'll have librosa 0.7.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, llvmlite, numba, librosa, opencv-contrib-python, opencv-python, torch, torchvision, tqdm\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: llvmlite 0.34.0\n","    Uninstalling llvmlite-0.34.0:\n","      Successfully uninstalled llvmlite-0.34.0\n","  Found existing installation: numba 0.51.2\n","    Uninstalling numba-0.51.2:\n","      Successfully uninstalled numba-0.51.2\n","  Found existing installation: librosa 0.8.0\n","    Uninstalling librosa-0.8.0:\n","      Successfully uninstalled librosa-0.8.0\n","  Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","  Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","  Found existing installation: torchvision 0.9.1+cu101\n","    Uninstalling torchvision-0.9.1+cu101:\n","      Successfully uninstalled torchvision-0.9.1+cu101\n","  Found existing installation: tqdm 4.41.1\n","    Uninstalling tqdm-4.41.1:\n","      Successfully uninstalled tqdm-4.41.1\n","Successfully installed librosa-0.7.0 llvmlite-0.31.0 numba-0.48.0 numpy-1.17.1 opencv-contrib-python-4.5.2.52 opencv-python-4.1.0.25 torch-1.1.0 torchvision-0.3.0 tqdm-4.45.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ey_bN4M6X_95","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621993787600,"user_tz":-330,"elapsed":6414,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"48f7f464-e57d-40b1-a154-33f8e472d7a5"},"source":["!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"Wav2Lip/face_detection/detection/sfd/s3fd.pth\""],"execution_count":12,"outputs":[{"output_type":"stream","text":["--2021-05-26 01:49:41--  https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\n","Resolving www.adrianbulat.com (www.adrianbulat.com)... 45.136.29.207\n","Connecting to www.adrianbulat.com (www.adrianbulat.com)|45.136.29.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 89843225 (86M) [application/octet-stream]\n","Saving to: ‘Wav2Lip/face_detection/detection/sfd/s3fd.pth’\n","\n","Wav2Lip/face_detect 100%[===================>]  85.68M  18.3MB/s    in 5.6s    \n","\n","2021-05-26 01:49:47 (15.4 MB/s) - ‘Wav2Lip/face_detection/detection/sfd/s3fd.pth’ saved [89843225/89843225]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlgBMoO14d4E","executionInfo":{"status":"ok","timestamp":1621993787602,"user_tz":-330,"elapsed":55,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"ffa738e4-1fcd-45f1-8fef-3deb5c678913"},"source":["%cd /content"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qUGuqlx4N_-0"},"source":["#### Function to upload our file to colab."]},{"cell_type":"code","metadata":{"id":"yAmXOaRjVLSj","executionInfo":{"status":"ok","timestamp":1621993787604,"user_tz":-330,"elapsed":44,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}}},"source":["def upload(savename):\n","  from google.colab import files\n","  uploaded = files.upload() \n","  for name, data in uploaded.items():\n","    with open(savename, 'wb') as f:\n","      f.write(data)\n","      print ('saved file', name)\n","  return name\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdIQfY2Kswcb"},"source":["# Now Lets do some deep fakes\n","\n","First we are getting ip from user and get to our local storage.\n"]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":109},"id":"GY9ZGD46UFW3","executionInfo":{"status":"ok","timestamp":1621994082110,"user_tz":-330,"elapsed":294549,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"3b6f10f2-560c-43b0-b444-9be14a9982f2"},"source":["upload('input_raw.mp4')"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-29213e8c-4f2f-40c1-8343-c4003707d067\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-29213e8c-4f2f-40c1-8343-c4003707d067\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving input.mp4 to input.mp4\n","saved file input.mp4\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'input.mp4'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":109},"id":"869zWKnwYflT","executionInfo":{"status":"ok","timestamp":1621994201867,"user_tz":-330,"elapsed":119768,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"df2f71fa-f21c-401b-c29e-1d426180e168"},"source":["upload('input_audio.mp4')"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-e2fb6dcd-f49c-4bb1-b7f3-944d0c8468df\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e2fb6dcd-f49c-4bb1-b7f3-944d0c8468df\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving videoplayback (2).mp4 to videoplayback (2).mp4\n","saved file videoplayback (2).mp4\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'videoplayback (2).mp4'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"JSejZZK-OZCs"},"source":["## From the main footage we are getting frames with faces."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZY9XcvpvE3ri","executionInfo":{"status":"ok","timestamp":1621994289640,"user_tz":-330,"elapsed":73738,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"8f680733-53f7-4a64-89b3-36590e99488f"},"source":["import cv2\n","face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_alt2.xml')\n","  \n","\n","# Create an object to read\n","# from camera\n","video = cv2.VideoCapture('/content/input_raw.mp4')\n","\n","# We need to check if camera\n","# is opened previously or not\n","if (video.isOpened() == False):\n","\tprint(\"Error reading video file\")\n","\n","# We need to set resolutions.\n","# so, convert them from float to integer.\n","frame_width = int(video.get(3))\n","frame_height = int(video.get(4))\n","\n","size = (frame_width, frame_height)\n","\n","# Below VideoWriter object will create\n","# a frame of above defined The output\n","# is stored in 'filename.avi' file.\n","result = cv2.VideoWriter('input.mp4',\n","\t\t\t\t\t\tcv2.VideoWriter_fourcc(*'MJPG'),\n","\t\t\t\t\t\t10, size)\n","\t\n","while(True):\n","  ret, frame = video.read()\n","\n","  if ret == True:\n","    p=0\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","# Detects faces of different sizes in the input image\n","    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n","    c=0\n","    for face in faces:\n","      c+=1\n","# Write the frame into the\n","# file 'filename.avi'\n","    if c!=0:\n","      result.write(frame)\n","\n","\n","  else:\n","    break\n","\n","# When everything done, release\n","# the video capture and video\n","# write objects\n","video.release()\n","result.release()\n","\t\n","\n","\n","print(\"The video was successfully saved\")\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The video was successfully saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Atxxnvu9a8kV","executionInfo":{"status":"ok","timestamp":1621994297800,"user_tz":-330,"elapsed":626,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"8080a2b1-32c7-40a8-d349-e9e4d27da978"},"source":["%cd /content/Wav2Lip/"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/content/Wav2Lip\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tMtV4_94OsBI"},"source":["### Setting model and doing lip_sync"]},{"cell_type":"markdown","metadata":{"id":"PjQavQ7RYjBG"},"source":["change this parameters as per need"]},{"cell_type":"code","metadata":{"id":"ejVwAF9iYdF9","executionInfo":{"status":"ok","timestamp":1621994302251,"user_tz":-330,"elapsed":509,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}}},"source":["static=False\n","fps=25\n","pads=[0,10,0,0]\n","face_det_batch_size =16\n","wav2lip_batch_size=128\n","resize_factor=1\n","crop=[0, -1, 0, -1]\n","box=[-1, -1, -1, -1]\n","rotate=False\n","nosmooth=False\n","img_size = 96"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM3gmw7GAqZC","executionInfo":{"status":"ok","timestamp":1621994436019,"user_tz":-330,"elapsed":128341,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"72eb0f8f-51b0-492c-a012-e7823e0a6f00"},"source":["from os import listdir, path\n","import numpy as np\n","import scipy, cv2, os, sys, argparse, audio\n","import json, subprocess, random, string\n","from tqdm import tqdm\n","from glob import glob\n","import torch, face_detection\n","from models import Wav2Lip\n","import platform\n","checkpoint_path=\"checkpoints/wav2lip_gan.pth\"\n","\n","face='/content/input.mp4'\n","audiof='/content/input_audio.mp4'\n","outfile='results/result_voice.mp4'\n","\n","\n","\n","if os.path.isfile(face) and face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n","\tstatic = True\n","\n","def get_smoothened_boxes(boxes, T):\n","\tfor i in range(len(boxes)):\n","\t\tif i + T > len(boxes):\n","\t\t\twindow = boxes[len(boxes) - T:]\n","\t\telse:\n","\t\t\twindow = boxes[i : i + T]\n","\t\tboxes[i] = np.mean(window, axis=0)\n","\treturn boxes\n","\n","def face_detect(images):\n","\tdetector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n","\t\t\t\t\t\t\t\t\t\t\tflip_input=False, device=device)\n","\n","\tbatch_size =face_det_batch_size\n","\t\n","\twhile 1:\n","\t\tpredictions = []\n","\t\ttry:\n","\t\t\tfor i in tqdm(range(0, len(images), batch_size)):\n","\t\t\t\tpredictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n","\t\texcept RuntimeError:\n","\t\t\tif batch_size == 1: \n","\t\t\t\traise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n","\t\t\tbatch_size //= 2\n","\t\t\tprint('Recovering from OOM error; New batch size: {}'.format(batch_size))\n","\t\t\tcontinue\n","\t\tbreak\n","\n","\tresults = []\n","\tpady1, pady2, padx1, padx2 = pads\n","\tfor rect, image in zip(predictions, images):\n","\t\tif rect is None:\n","\t\t\tcv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n","\t\t\traise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n","\n","\t\ty1 = max(0, rect[1] - pady1)\n","\t\ty2 = min(image.shape[0], rect[3] + pady2)\n","\t\tx1 = max(0, rect[0] - padx1)\n","\t\tx2 = min(image.shape[1], rect[2] + padx2)\n","\t\t\n","\t\tresults.append([x1, y1, x2, y2])\n","\n","\tboxes = np.array(results)\n","\tif not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n","\tresults = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n","\n","\tdel detector\n","\treturn results \n","\n","def datagen(frames, mels):\n","\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n","\n","\tif box[0] == -1:\n","\t\tif not static:\n","\t\t\tface_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n","\t\telse:\n","\t\t\tface_det_results = face_detect([frames[0]])\n","\telse:\n","\t\tprint('Using the specified bounding box instead of face detection...')\n","\t\ty1, y2, x1, x2 = box\n","\t\tface_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames]\n","\n","\tfor i, m in enumerate(mels):\n","\t\tidx = 0 if static else i%len(frames)\n","\t\tframe_to_save = frames[idx].copy()\n","\t\tface, coords = face_det_results[idx].copy()\n","\n","\t\tface = cv2.resize(face, (img_size,img_size))\n","\t\t\t\n","\t\timg_batch.append(face)\n","\t\tmel_batch.append(m)\n","\t\tframe_batch.append(frame_to_save)\n","\t\tcoords_batch.append(coords)\n","\n","\t\tif len(img_batch) >= wav2lip_batch_size:\n","\t\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n","\n","\t\t\timg_masked = img_batch.copy()\n","\t\t\timg_masked[:,img_size//2:] = 0\n","\n","\t\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n","\t\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n","\n","\t\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n","\t\t\timg_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n","\n","\tif len(img_batch) > 0:\n","\t\timg_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n","\n","\t\timg_masked = img_batch.copy()\n","\t\timg_masked[:, img_size//2:] = 0\n","\n","\t\timg_batch = np.concatenate((img_masked, img_batch), axis=3) / 255.\n","\t\tmel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n","\n","\t\tyield img_batch, mel_batch, frame_batch, coords_batch\n","\n","mel_step_size = 16\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} for inference.'.format(device))\n","\n","def _load(checkpoint_path):\n","\tif device == 'cuda':\n","\t\tcheckpoint = torch.load(checkpoint_path)\n","\telse:\n","\t\tcheckpoint = torch.load(checkpoint_path,\n","\t\t\t\t\t\t\t\tmap_location=lambda storage, loc: storage)\n","\treturn checkpoint\n","\n","def load_model(path):\n","\tmodel = Wav2Lip()\n","\tprint(\"Load checkpoint from: {}\".format(path))\n","\tcheckpoint = _load(path)\n","\ts = checkpoint[\"state_dict\"]\n","\tnew_s = {}\n","\tfor k, v in s.items():\n","\t\tnew_s[k.replace('module.', '')] = v\n","\tmodel.load_state_dict(new_s)\n","\n","\tmodel = model.to(device)\n","\treturn model.eval()\n","# MAIN FUNCTION\n","if not os.path.isfile(face):\n","  raise ValueError('--face argument must be a valid path to video/image file')\n","\n","elif face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n","  full_frames = [cv2.imread(face)]\n","  fps = fps\n","\n","else:\n","  video_stream = cv2.VideoCapture(face)\n","  fps = video_stream.get(cv2.CAP_PROP_FPS)\n","\n","  print('Reading video frames...')\n","\n","  full_frames = []\n","  while 1:\n","    still_reading, frame = video_stream.read()\n","    if not still_reading:\n","      video_stream.release()\n","      break\n","    if resize_factor > 1:\n","      frame = cv2.resize(frame, (frame.shape[1]//resize_factor, frame.shape[0]//resize_factor))\n","\n","    if rotate:\n","      frame = cv2.rotate(frame, cv2.cv2.ROTATE_90_CLOCKWISE)\n","\n","    y1, y2, x1, x2 =crop\n","    if x2 == -1: x2 = frame.shape[1]\n","    if y2 == -1: y2 = frame.shape[0]\n","\n","    frame = frame[y1:y2, x1:x2]\n","\n","    full_frames.append(frame)\n","\n","print (\"Number of frames available for inference: \"+str(len(full_frames)))\n","\n","if not audiof.endswith('.wav'):\n","  print('Extracting raw audio...')\n","  command = 'ffmpeg -y -i {} -strict -2 {}'.format(audiof, 'temp/temp.wav')\n","\n","  subprocess.call(command, shell=True)\n","  audiof = 'temp/temp.wav'\n","\n","wav = audio.load_wav(audiof, 16000)\n","mel = audio.melspectrogram(wav)\n","print(mel.shape)\n","\n","if np.isnan(mel.reshape(-1)).sum() > 0:\n","  raise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')\n","\n","mel_chunks = []\n","mel_idx_multiplier = 80./fps \n","i = 0\n","while 1:\n","  start_idx = int(i * mel_idx_multiplier)\n","  if start_idx + mel_step_size > len(mel[0]):\n","    mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n","    break\n","  mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n","  i += 1\n","\n","print(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n","\n","full_frames = full_frames[:len(mel_chunks)]\n","\n","batch_size =wav2lip_batch_size\n","gen = datagen(full_frames.copy(), mel_chunks)\n","\n","for i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n","                    total=int(np.ceil(float(len(mel_chunks))/batch_size)))):\n","  if i == 0:\n","    model = load_model(checkpoint_path)\n","    print (\"Model loaded\")\n","\n","    frame_h, frame_w = full_frames[0].shape[:-1]\n","    out = cv2.VideoWriter('temp/result.avi', \n","                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n","\n","  img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n","  mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n","\n","  with torch.no_grad():\n","    pred = model(mel_batch, img_batch)\n","\n","  pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n","  \n","  for p, f, c in zip(pred, frames, coords):\n","    y1, y2, x1, x2 = c\n","    p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n","\n","    f[y1:y2, x1:x2] = p\n","    out.write(f)\n","\n","out.release()\n","\n","command = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(audiof, 'temp/result.avi', outfile)\n","subprocess.call(command, shell=platform.system() != 'Windows')\n","\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Using cuda for inference.\n","Reading video frames...\n","Number of frames available for inference: 859\n","Extracting raw audio...\n","(80, 3716)\n","Length of mel chunks: 464\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 0/4 [00:00<?, ?it/s]\n","  0%|          | 0/29 [00:00<?, ?it/s]\n","\n","  0%|          | 0/58 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Recovering from OOM error; New batch size: 8\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  2%|▏         | 1/58 [00:08<07:57,  8.38s/it]\u001b[A\n","  3%|▎         | 2/58 [00:09<05:47,  6.21s/it]\u001b[A\n","  5%|▌         | 3/58 [00:10<04:18,  4.69s/it]\u001b[A\n","  7%|▋         | 4/58 [00:11<03:16,  3.64s/it]\u001b[A\n","  9%|▊         | 5/58 [00:13<02:33,  2.89s/it]\u001b[A\n"," 10%|█         | 6/58 [00:14<02:03,  2.37s/it]\u001b[A\n"," 12%|█▏        | 7/58 [00:15<01:42,  2.01s/it]\u001b[A\n"," 14%|█▍        | 8/58 [00:16<01:27,  1.76s/it]\u001b[A\n"," 16%|█▌        | 9/58 [00:17<01:17,  1.58s/it]\u001b[A\n"," 17%|█▋        | 10/58 [00:18<01:09,  1.46s/it]\u001b[A\n"," 19%|█▉        | 11/58 [00:19<01:04,  1.37s/it]\u001b[A\n"," 21%|██        | 12/58 [00:21<01:00,  1.31s/it]\u001b[A\n"," 22%|██▏       | 13/58 [00:22<00:56,  1.27s/it]\u001b[A\n"," 24%|██▍       | 14/58 [00:23<00:54,  1.23s/it]\u001b[A\n"," 26%|██▌       | 15/58 [00:24<00:52,  1.21s/it]\u001b[A\n"," 28%|██▊       | 16/58 [00:25<00:50,  1.20s/it]\u001b[A\n"," 29%|██▉       | 17/58 [00:26<00:48,  1.19s/it]\u001b[A\n"," 31%|███       | 18/58 [00:28<00:47,  1.18s/it]\u001b[A\n"," 33%|███▎      | 19/58 [00:29<00:45,  1.18s/it]\u001b[A\n"," 34%|███▍      | 20/58 [00:30<00:44,  1.18s/it]\u001b[A\n"," 36%|███▌      | 21/58 [00:31<00:43,  1.18s/it]\u001b[A\n"," 38%|███▊      | 22/58 [00:32<00:42,  1.18s/it]\u001b[A\n"," 40%|███▉      | 23/58 [00:34<00:41,  1.18s/it]\u001b[A\n"," 41%|████▏     | 24/58 [00:35<00:40,  1.18s/it]\u001b[A\n"," 43%|████▎     | 25/58 [00:36<00:38,  1.18s/it]\u001b[A\n"," 45%|████▍     | 26/58 [00:37<00:37,  1.18s/it]\u001b[A\n"," 47%|████▋     | 27/58 [00:38<00:36,  1.18s/it]\u001b[A\n"," 48%|████▊     | 28/58 [00:39<00:35,  1.18s/it]\u001b[A\n"," 50%|█████     | 29/58 [00:41<00:34,  1.18s/it]\u001b[A\n"," 52%|█████▏    | 30/58 [00:42<00:33,  1.19s/it]\u001b[A\n"," 53%|█████▎    | 31/58 [00:43<00:32,  1.19s/it]\u001b[A\n"," 55%|█████▌    | 32/58 [00:44<00:30,  1.18s/it]\u001b[A\n"," 57%|█████▋    | 33/58 [00:45<00:29,  1.19s/it]\u001b[A\n"," 59%|█████▊    | 34/58 [00:47<00:28,  1.19s/it]\u001b[A\n"," 60%|██████    | 35/58 [00:48<00:27,  1.19s/it]\u001b[A\n"," 62%|██████▏   | 36/58 [00:49<00:26,  1.19s/it]\u001b[A\n"," 64%|██████▍   | 37/58 [00:50<00:25,  1.19s/it]\u001b[A\n"," 66%|██████▌   | 38/58 [00:51<00:23,  1.20s/it]\u001b[A\n"," 67%|██████▋   | 39/58 [00:53<00:22,  1.19s/it]\u001b[A\n"," 69%|██████▉   | 40/58 [00:54<00:21,  1.20s/it]\u001b[A\n"," 71%|███████   | 41/58 [00:55<00:20,  1.20s/it]\u001b[A\n"," 72%|███████▏  | 42/58 [00:56<00:19,  1.20s/it]\u001b[A\n"," 74%|███████▍  | 43/58 [00:57<00:18,  1.20s/it]\u001b[A\n"," 76%|███████▌  | 44/58 [00:59<00:16,  1.21s/it]\u001b[A\n"," 78%|███████▊  | 45/58 [01:00<00:15,  1.21s/it]\u001b[A\n"," 79%|███████▉  | 46/58 [01:01<00:14,  1.21s/it]\u001b[A\n"," 81%|████████  | 47/58 [01:02<00:13,  1.21s/it]\u001b[A\n"," 83%|████████▎ | 48/58 [01:03<00:12,  1.20s/it]\u001b[A\n"," 84%|████████▍ | 49/58 [01:05<00:10,  1.21s/it]\u001b[A\n"," 86%|████████▌ | 50/58 [01:06<00:09,  1.21s/it]\u001b[A\n"," 88%|████████▊ | 51/58 [01:07<00:08,  1.20s/it]\u001b[A\n"," 90%|████████▉ | 52/58 [01:08<00:07,  1.20s/it]\u001b[A\n"," 91%|█████████▏| 53/58 [01:09<00:05,  1.19s/it]\u001b[A\n"," 93%|█████████▎| 54/58 [01:11<00:04,  1.19s/it]\u001b[A\n"," 95%|█████████▍| 55/58 [01:12<00:03,  1.19s/it]\u001b[A\n"," 97%|█████████▋| 56/58 [01:13<00:02,  1.19s/it]\u001b[A\n"," 98%|█████████▊| 57/58 [01:14<00:01,  1.19s/it]\u001b[A\n","100%|██████████| 58/58 [01:15<00:00,  1.31s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Load checkpoint from: checkpoints/wav2lip_gan.pth\n","Model loaded\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 4/4 [01:40<00:00, 25.20s/it]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"sfJhEC_QO3m6"},"source":["### Displaying the result"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":246,"output_embedded_package_id":"1zZXe7Vw1l20Yu0_JxEbHQOUDnvHmZe95"},"id":"gAxiz5A75rOb","executionInfo":{"status":"ok","timestamp":1621994445101,"user_tz":-330,"elapsed":4248,"user":{"displayName":"Rithish kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjME8_kNalfGY9iFc4xUaumwUwkx9zhau1IhKy2Fg=s64","userId":"09250092512725729505"}},"outputId":"9357e021-24ea-4085-fe56-5184fedd7830"},"source":["from IPython.display import HTML\n","from base64 import b64encode\n","mp4 = open('/content/Wav2Lip/results/result_voice.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"],"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}